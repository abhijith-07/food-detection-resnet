{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwfiD8VFGfz+3oLlzdWTHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijith-07/food-detection-resnet/blob/main/food_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qGOythiXuXEN"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "from dropblock import DropBlock2D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class self_attention(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, dk, dq, dv, Nh):\n",
        "        super(self_attention, self).__init__()\n",
        "        self.Cin = in_channels\n",
        "        self.Cout = out_channels\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.Nh = Nh\n",
        "\n",
        "        self.k = int(self.dk * self.Cin)\n",
        "        self.q = int(self.dq * self.Cin)\n",
        "        self.v = int(self.dv * self.Cin)\n",
        "\n",
        "        self.kqv_conv = nn.Sequential(\n",
        "            nn.Conv2d(self.Cin, self.k+self.q+self.v, kernel_size=1, stride=1, padding=0),\n",
        "            #nn.BatchNorm2d(self.k+self.q+self.v,self.k+self.q+self.v)\n",
        "        )\n",
        "        self.attn = nn.Conv2d(self.v, self.Cout, kernel_size=1, stride=1)\n",
        "\n",
        "    def split_heads_2d(self, x, Nh):\n",
        "        batch, channels, height, width = x.size()\n",
        "        ret_shape = (batch, Nh, channels // Nh, height, width)\n",
        "        split = torch.reshape(x, ret_shape)\n",
        "        return split\n",
        "\n",
        "    #shape of flat_q: (N, Nh, dq//Nh, H*W)\n",
        "    #shape of q:      (N, Nh, dq//Nh, H, W)\n",
        "    def compute_flat_qkv(self, x, dq, dk, dv, Nh):\n",
        "        qkv = self.kqv_conv(x)\n",
        "        N, _, H, W = qkv.size()\n",
        "        q, k, v = torch.split(qkv, [dq, dk, dv], dim=1)\n",
        "        q = self.split_heads_2d(q, Nh)\n",
        "        k = self.split_heads_2d(k, Nh)\n",
        "        v = self.split_heads_2d(v, Nh)\n",
        "\n",
        "        dkh = dk // Nh\n",
        "        q *= dkh ** -0.5\n",
        "        flat_q = torch.reshape(q, (N, Nh, dq // Nh, H * W))\n",
        "        flat_k = torch.reshape(k, (N, Nh, dk // Nh, H * W))\n",
        "        flat_v = torch.reshape(v, (N, Nh, dv // Nh, H * W))\n",
        "\n",
        "        return flat_q, flat_k, flat_v, q, k, v\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch, N, H, W = inputs.shape\n",
        "        #print(inputs.shape)\n",
        "        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(inputs, self.q, self.k,self.v,self.Nh)\n",
        "        #print(flat_q.shape)\n",
        "        logits = torch.matmul(flat_q.transpose(2, 3), flat_k)\n",
        "        weights = F.softmax(logits, dim=1)\n",
        "        #print(weights.shape)\n",
        "        #result = weights.cpu().detach().numpy()\n",
        "        #np.save(\"visual/matrix\"+str(H), result)\n",
        "        #print(weights.shape)\n",
        "        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))\n",
        "        attn_out = torch.reshape(attn_out, (batch, self.Nh, self.v // self.Nh, H, W))\n",
        "        #print(attn_out.shape)\n",
        "        attn_out = torch.reshape(attn_out, (batch, self.Nh * (self.v // self.Nh), H, W))\n",
        "        #print(attn_out.shape)\n",
        "        attn_out = self.attn(attn_out)\n",
        "        #print(attn_out.shape)\n",
        "\n",
        "        return attn_out\n"
      ],
      "metadata": {
        "id": "pJkrJRX_zNz2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,\n",
        "                              stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,\n",
        "                                 momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU() if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XraaLANWy0Pw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PRENet(nn.Module):\n",
        "    def __init__(self, model, feature_size, classes_num):\n",
        "        super(PRENet, self).__init__()\n",
        "\n",
        "        self.features = model\n",
        "\n",
        "        self.num_ftrs = 2048 * 1 * 1\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "\n",
        "        self.dk = 0.5\n",
        "        self.dq = 0.5\n",
        "        self.dv = 0.5\n",
        "        self.Nh = 8\n",
        "\n",
        "\n",
        "        self.classifier_concat = nn.Sequential(\n",
        "            nn.BatchNorm1d(1024 * 5),\n",
        "            nn.Linear(1024 * 5, feature_size),\n",
        "            nn.BatchNorm1d(feature_size),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(feature_size, classes_num),\n",
        "        )\n",
        "\n",
        "        self.conv_block0 = nn.Sequential(\n",
        "            BasicConv(self.num_ftrs // 8, feature_size, kernel_size=1, stride=1, padding=0, relu=True),\n",
        "            BasicConv(feature_size, self.num_ftrs // 2, kernel_size=3, stride=1, padding=1, relu=True)\n",
        "        )\n",
        "        self.classifier0 = nn.Sequential(\n",
        "            nn.BatchNorm1d(self.num_ftrs // 2),\n",
        "            nn.Linear(self.num_ftrs // 2, feature_size),\n",
        "            nn.BatchNorm1d(feature_size),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(feature_size, classes_num),\n",
        "        )\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            BasicConv(self.num_ftrs//4, feature_size, kernel_size=1, stride=1, padding=0, relu=True),\n",
        "            BasicConv(feature_size, self.num_ftrs//2, kernel_size=3, stride=1, padding=1, relu=True)\n",
        "        )\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.BatchNorm1d(self.num_ftrs//2),\n",
        "            nn.Linear(self.num_ftrs//2, feature_size),\n",
        "            nn.BatchNorm1d(feature_size),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(feature_size, classes_num),\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            BasicConv(self.num_ftrs//2, feature_size, kernel_size=1, stride=1, padding=0, relu=True),\n",
        "            BasicConv(feature_size, self.num_ftrs//2, kernel_size=3, stride=1, padding=1, relu=True)\n",
        "        )\n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(self.num_ftrs//2),\n",
        "            nn.Linear(self.num_ftrs//2, feature_size),\n",
        "            nn.BatchNorm1d(feature_size),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(feature_size, classes_num),\n",
        "        )\n",
        "\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            BasicConv(self.num_ftrs, feature_size, kernel_size=1, stride=1, padding=0, relu=True),\n",
        "            BasicConv(feature_size, self.num_ftrs//2, kernel_size=3, stride=1, padding=1, relu=True)\n",
        "        )\n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.BatchNorm1d(self.num_ftrs//2),\n",
        "            nn.Linear(self.num_ftrs//2, feature_size),\n",
        "            nn.BatchNorm1d(feature_size),\n",
        "            nn.ELU(inplace=True),\n",
        "            nn.Linear(feature_size, classes_num),\n",
        "        )\n",
        "\n",
        "        self.Avgmax = nn.AdaptiveMaxPool2d(output_size=(1,1))\n",
        "\n",
        "        self.attn1_1 = self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn2_2 = self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn3_3 = self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "\n",
        "        '''\n",
        "        self.attn1_2 = layer_self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn1_3 = layer_self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn2_3 = layer_self_attention(self.num_ftrs // 2,self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "\n",
        "        self.attn2_1 = layer_self_attention(self.num_ftrs // 2, self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn3_1 = layer_self_attention(self.num_ftrs // 2, self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        self.attn3_2 = layer_self_attention(self.num_ftrs // 2, self.num_ftrs // 2, self.dk, self.dq, self.dv, self.Nh)\n",
        "        '''\n",
        "\n",
        "        self.sconv1 = nn.Conv2d((self.num_ftrs // 2), self.num_ftrs // 2, kernel_size= 3, padding= 1)\n",
        "        self.sconv2 = nn.Conv2d((self.num_ftrs // 2), self.num_ftrs // 2, kernel_size= 3, padding= 1)\n",
        "        self.sconv3 = nn.Conv2d((self.num_ftrs // 2), self.num_ftrs // 2, kernel_size= 3, padding= 1)\n",
        "        self.drop_block = DropBlock2D(block_size=3, drop_prob=0.5)\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        xf1, xf2, xf3, xf4, xf5, xn = self.features(x)\n",
        "        batch_size, _, _, _ = x.shape\n",
        "\n",
        "        #get feature pyramid\n",
        "        xl1 = self.conv_block1(xf3)\n",
        "        xl2 = self.conv_block2(xf4)\n",
        "        xl3 = self.conv_block3(xf5)\n",
        "\n",
        "        xk1 = self.Avgmax(xl1)\n",
        "        xk1 = xk1.view(xk1.size(0), -1)\n",
        "        xc1 = self.classifier1(xk1)\n",
        "\n",
        "        xk2 = self.Avgmax(xl2)\n",
        "        xk2 = xk2.view(xk2.size(0), -1)\n",
        "        xc2 = self.classifier2(xk2)\n",
        "\n",
        "        xk3 = self.Avgmax(xl3)\n",
        "        xk3 = xk3.view(xk3.size(0), -1)\n",
        "        xc3 = self.classifier3(xk3)\n",
        "\n",
        "\n",
        "        if label:\n",
        "            # xs1_2 means that using x2 to strength x1\n",
        "            #(batch, 1024, 56, 56)\n",
        "            xs1 = self.attn1_1(xl1)\n",
        "            #xs1_2 = self.attn1_2(xl1, xl2)\n",
        "            #xs1_3 = self.attn1_3(xl1, xl3)\n",
        "            # (batch, 1024, 28, 28)\n",
        "            xs2 = self.attn1_1(xl2)\n",
        "            #xs2_3 = self.attn2_3(xl2, xl3)\n",
        "            #xs2_1 = self.attn2_1(xl2, xl1)\n",
        "            # (batch, 1024, 14, 14)\n",
        "            xs3 = self.attn1_1(xl3)\n",
        "            #xs3_1 = self.attn2_1(xl3, xl1)\n",
        "            #xs3_2 = self.attn2_1(xl3, xl2)\n",
        "\n",
        "            #xr1 = self.drop_block(self.sconv1(torch.cat([xs1,xs1_2,xs1_3], dim=1)))\n",
        "            #xr2 = self.drop_block(self.sconv2(torch.cat([xs2,xs2_3,xs2_1], dim=1)))\n",
        "            #xr3 = self.drop_block(self.sconv3(torch.cat([xs3,xs3_1,xs3_2], dim=1)))\n",
        "            xr1 = self.drop_block(self.sconv1(xs1))\n",
        "            xr2 = self.drop_block(self.sconv2(xs2))\n",
        "            xr3 = self.drop_block(self.sconv3(xs3))\n",
        "\n",
        "            xm1 = self.Avgmax(xr1)\n",
        "            xm1 = xm1.view(xm1.size(0), -1)\n",
        "            #print(np.argmax(F.softmax(xm1, dim=1).cpu().detach().numpy(),axis=1))\n",
        "            #input()\n",
        "\n",
        "            xm2 = self.Avgmax(xr2)\n",
        "            xm2 = xm2.view(xm2.size(0), -1)\n",
        "            #print(np.argmax(F.softmax(xm2, dim=1).cpu().detach().numpy(),axis=1))\n",
        "            #input()\n",
        "\n",
        "            xm3 = self.Avgmax(xr3)\n",
        "            xm3 = xm3.view(xm3.size(0), -1)\n",
        "            #print(np.argmax(F.softmax(xm3, dim=1).cpu().detach().numpy(),axis=1))\n",
        "            #input()\n",
        "\n",
        "            x_concat = torch.cat((xm1, xm2, xm3, xn), dim=1)\n",
        "            x_concat = self.classifier_concat(x_concat)\n",
        "        else:\n",
        "            x_concat = torch.cat((xk1, xk2, xk3, xn), dim=1)\n",
        "            x_concat = self.classifier_concat(x_concat)\n",
        "\n",
        "        #get origal feature vector\n",
        "\n",
        "\n",
        "        return xk1, xk2, xk3, x_concat, xc1, xc2, xc3\n"
      ],
      "metadata": {
        "id": "4v4bkKhWwYbS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "xIj1CoOFzA-4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = os.listdir('dataset/train_set')\n",
        "categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ6wYQNu5ay_",
        "outputId": "0514ce09-4734-4b5f-83ae-af301b676d14"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bisibelebath',\n",
              " 'biriyani',\n",
              " 'butternaan',\n",
              " 'chaat',\n",
              " '.ipynb_checkpoints',\n",
              " 'chappati',\n",
              " 'dosa']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 256\n",
        "\n",
        "def get_data(data_dir):\n",
        "    print(\"Data Dir:\", data_dir)\n",
        "    features = []\n",
        "    labels = []\n",
        "    for category in categories:\n",
        "        path = os.path.join(data_dir, category)\n",
        "        class_num = categories.index(category)\n",
        "        print(\"Class number: \", class_num)\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img))\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
        "\n",
        "                features.append(resized_arr)\n",
        "                labels.append(class_num)\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(features), np.array(labels)"
      ],
      "metadata": {
        "id": "re3HsDu05pOf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_features, train_labels) = get_data('dataset/train_set')\n",
        "train_features.shape, train_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGllAXm35t10",
        "outputId": "c902ec70-930c-4957-fb7b-8ab73454bb0a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Dir: dataset/train_set\n",
            "Class number:  0\n",
            "Class number:  1\n",
            "Class number:  2\n",
            "Class number:  3\n",
            "Class number:  4\n",
            "Class number:  5\n",
            "Class number:  6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((420, 256, 256, 3), (420,))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(test_features, test_labels) = get_data('dataset/train_set')\n",
        "test_features.shape, test_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZIaDEJW6ZKQ",
        "outputId": "173f944e-373b-461b-f52e-74535b7377ab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Dir: dataset/train_set\n",
            "Class number:  0\n",
            "Class number:  1\n",
            "Class number:  2\n",
            "Class number:  3\n",
            "Class number:  4\n",
            "Class number:  5\n",
            "Class number:  6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((420, 256, 256, 3), (420,))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  resnet_model = models.resnet50(pretrained=True)\n",
        "except Exception as e:\n",
        "  print(\"Exception: \", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlCmDyL-7r50",
        "outputId": "56809fda-6a13-43f6-ab32-36df960fc73d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = 6\n",
        "prenet_model = PRENet(resnet_model, 512, num_class)"
      ],
      "metadata": {
        "id": "Ng7UNrFII67N"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjonbJARQI9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}